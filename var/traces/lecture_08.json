{
  "files": {
    "lecture_08.py": "import torch\nimport time\nimport os\nfrom typing import List, Callable\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.distributed.fsdp\nfrom execute_util import text, image, link, system_text\nfrom torch_util import get_device\nfrom lecture_util import article_link\nfrom lecture_08_utils import spawn, int_divide, summarize_tensor, get_init_params\n\ndef main():\n    text(\"Last week: parallelism within a single GPU\")\n    text(\"This week: parallelism across multiple GPUs\")\n    image(\"images/gpu-node-overview.png\", width=600)\n\n    text(\"In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).\")\n    text(\"Unifying theme: orchestrate computation to avoid data transfer bottlenecks\")\n\n    text(\"Last week: reduce memory accesses via fusion/tiling\")\n    text(\"This week: reduce communication across GPUs/nodes via replication/sharding\")\n\n    text(\"Generalized hierarchy (from small/fast to big/slow):\")\n    text(\"- Single node, single GPU: L1 cache / shared memory\")\n    text(\"- Single node, single GPU: HBM\")\n    text(\"- Single node, multi-GPU: NVLink\")\n    text(\"- Multi-node, multi-GPU: NVSwitch\")\n\n    text(\"## Part 1: building blocks of distributed communication/computation\")\n    hardware()                 # How nodes actually communicate\n    collective_operations()    # Conceptual programming interface\n    torch_distributed()        # How this is implemented in NCCL/PyTorch\n    benchmarking()             # Estimate NCCL bandwidth\n\n    text(\"## Part 2: distributed training\")\n    text(\"Walk through bare-bones implementations of each strategy on deep MLPs.\")\n    text(\"Recall that MLPs are the compute bottleneck in Transformers.\")\n    data_parallelism()         # Cut up along the batch dimension\n    tensor_parallelism()       # Cut up along the width dimension\n    pipeline_parallelism()     # Cut up along the depth dimension\n\n    text(\"## Summary\")\n\n    text(\"The game: trading off\")\n    text(\"- memory usage (store locally) and\")\n    text(\"- communication (send across GPUs)\")\n\n    text(\"- Hardware is getting faster, but will always have this hierarchical structure\")\n    text(\"- Many ways to parallelize: data, tensor/expert, pipeline, sequence\")\n\n\ndef hardware():\n    text(\"## Single GPU\")\n    image(\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg\", width=1000)\n    text(\"Memory bandwidth for HBM for H100 NVL is 3.9 TB/s \"), article_link(\"https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")\n\n    text(\"## Multi-node, multi-GPU\")\n\n    text(\"Traditionally:\")\n    image(\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs42774-021-00098-3/MediaObjects/42774_2021_98_Fig1_HTML.png?as=webp\", width=400)\n    text(\"- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) \"), article_link(\"https://en.wikipedia.org/wiki/PCI_Express\")\n    text(\"- GPUs on different nodes communicate via Ethernet (~200 MB/sec)\")\n\n    text(\"Both are too slow...\")\n    text(\"Key hardware advance: have GPUs connect *directly*, bypassing CPU\")\n\n    text(\"## InfiniBand\")\n\n    text(\"Standard developed in 1999; Mellanox created InfiniBand hardware, acquired by NVIDIA in 2019\")\n    text(\"Idea: Remote Direct Memory Access (RDMA) to connect nodes directly\")\n    image(\"https://lambdalabs.com/hubfs/Imported_Blog_Media/nvlink-diagram-update.png\", width=600)\n\n    text(\"## NVLink/NVSwitch\")\n\n    text(\"NVIDIA developed proprietary protocol since 2014\")\n    text(\"4.5x more bandwidth than InfiniBand \"), article_link(\"https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/\")\n\n    text(\"Within a node: NVLink connects GPUs directly, bypass CPU\")\n    image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/08/NVLink-generations-1.png\", width=800)\n\n    text(\"Across nodes: NVSwitch connects GPUs directly, bypass Ethernet\")\n    image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/08/NVLink-all-to-all-connectivity-1.png\", width=800)\n\n    text(\"H100: 18 NVLink 4.0 links => 900GB/sec\")\n\n    text(\"Let's check what our hardware setup is. \"), article_link(\"https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc\")\n\n    if torch.cuda.is_available():\n        system_text([\"nvidia-smi\", \"topo\", \"-m\"])\n        text(\"Note GPUs are connected via NV18, also connected to NICs (for PCIe)\")\n\n\ndef collective_operations():\n    text(\"Collective operations are the conceptual primitives used for distributed programming \"), article_link(\"https://en.wikipedia.org/wiki/Collective_operation\")\n\n    text(\"- Collective means that specify communication pattern across many (e.g., 256) nodes\")\n    text(\"- These are classic in the parallel programming literature from the 1980s\")\n    text(\"- For SIMD (Single Instruction, Multiple Data) parallelism\")\n    text(\"- Better/faster abstraction than managing point-to-point communication yourself\")\n\n    text(\"Terminology:\")\n    text(\"- **Rank**: a device (e.g., GPU)\")\n    text(\"- **World size**: number of devices\")\n\n    text(\"## Broadcast\"), image(\"https://pytorch.org/tutorials/_images/broadcast.png\", width=400)\n\n    text(\"## Scatter\"), image(\"https://pytorch.org/tutorials/_images/scatter.png\", width=400)\n\n    text(\"## Gather\"), image(\"https://pytorch.org/tutorials/_images/gather.png\", width=400)\n\n    text(\"## Reduce\"), image(\"https://pytorch.org/tutorials/_images/reduce.png\", width=400)\n\n    text(\"## All-gather\"), image(\"https://pytorch.org/tutorials/_images/all_gather.png\", width=400)\n\n    text(\"## Reduce-scatter\"), image(\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png\", width=400)\n\n    text(\"## All-reduce = reduce-scatter + all-gather\"), image(\"https://pytorch.org/tutorials/_images/all_reduce.png\", width=400)\n\n    text(\"Way to remember:\")\n    text(\"- Reduce: performs some associative/commutative operation (sum, min, max)\")\n    text(\"- Broadcast/scatter is inverse of gather\")\n    text(\"- All: means destination is all devices\")\n\n\ndef torch_distributed():\n    text(\"## PyTorch distributed library (`torch.distributed`) \")\n    link(\"[Documentation]\", url=\"https://pytorch.org/docs/stable/distributed.html\")\n\n    text(\"- Provides clean interface for collective operations (e.g., `all_reduce`)\")\n    text(\"- Backends: gloo (CPU), nccl (GPU)\")\n    text(\"- Also supports higher-level abstractions (e.g., `FullyShardedDataParallel`)\")\n\n    text(\"## NVIDIA Collective Communication Library (NCCL)\")\n\n    text(\"NCCL translates collective operations into low-level packets that are sent between GPUs. \"), link(title=\"[talk]\", url=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/\")\n    text(\"- Detect toplogy of hardware (e.g., number of nodes, switches, NVLink/PCIe)\")\n    text(\"- Optimize the path between ranks; ring (good bandwidth), tree (good latency)\")\n    image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2019/02/DBtree.png\", width=400)\n    text(\"- Launches CUDA kernels to send/receive data\")\n\n    # Walk through examples\n    link(title=\"[stdout]\", url=\"var/lecture_08_stdout.txt\")\n    spawn(collective_operations_main, world_size=4)\n\n\ndef collective_operations_main(rank: int, world_size: int):\n    \"\"\"Try out some collective operations.\"\"\"\n    # Note: this function is running asynchronously for each process (world_size)\n\n    setup(rank, world_size)\n\n    # All-reduce\n    dist.barrier()  # Waits for all processes to get to this point\n\n    tensor = torch.tensor([0., 1, 2, 3], device=get_device(rank)) + rank  # Both input and output\n\n    print(f\"Rank {rank} [before all-reduce]: {tensor}\")\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)  # Modifies tensor in place\n    print(f\"Rank {rank} [after all-reduce]: {tensor}\")\n\n    # Reduce-scatter\n    dist.barrier()\n\n    input = torch.arange(world_size, dtype=torch.float32, device=get_device(rank)) + rank  # Input\n    output = torch.empty(1, device=get_device(rank))  # Allocate output\n\n    print(f\"Rank {rank} [before reduce-scatter]: input = {input}, output = {output}\")\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    print(f\"Rank {rank} [after reduce-scatter]: input = {input}, output = {output}\")\n\n    # All-gather\n    dist.barrier()\n\n    input = output  # Input is the output of reduce-scatter\n    output = torch.empty(world_size, device=get_device(rank))  # Allocate output\n\n    print(f\"Rank {rank} [before all-gather]: input = {input}, output = {output}\")\n    dist.all_gather_into_tensor(output_tensor=output, input_tensor=input, async_op=False)\n    print(f\"Rank {rank} [after all-gather]: input = {input}, output = {output}\")\n\n    text(\"Recall that all-reduce = reduce-scatter + all-gather!\")\n\n    cleanup()\n\n\ndef benchmarking():\n    text(\"## Benchmarking\"), link(\"https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py\")\n\n    text(\"Let's see how fast commmunication happens (will restrict to just one node).\")\n\n    text(\"### All-reduce\")\n    spawn(all_reduce, world_size=2, num_elements=1024**2)\n    spawn(all_reduce, world_size=4, num_elements=1024**2)\n\n    text(\"### Reduce-scatter\")\n    spawn(reduce_scatter, world_size=2, num_elements=1024**2)\n    spawn(reduce_scatter, world_size=4, num_elements=1024**2)\n\n    link(title=\"Reference on reasoning about operations\", url=\"https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce\")\n\n\ndef all_reduce(rank: int, world_size: int, num_elements: int):\n    setup(rank, world_size)\n\n    # Create tensor\n    tensor = torch.randn(num_elements, device=get_device(rank))\n\n    # Warmup\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n\n    # All reduce\n    start_time = time.time()\n    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n    end_time = time.time()\n\n    duration = end_time - start_time\n    print(f\"[all_reduce] Rank {rank}: all_reduce(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")\n\n    # Estimate the bandwidth\n    size_bytes = tensor.element_size() * tensor.numel()\n    sent_bytes = size_bytes * 2 * (world_size - 1)  # 2x because of send and receive\n    total_duration = world_size * duration\n    bandwidth = sent_bytes / total_duration\n    print(f\"[all_reduce] Rank {rank}: all_reduce estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")\n\n    cleanup()\n\n\ndef reduce_scatter(rank: int, world_size: int, num_elements: int):\n    setup(rank, world_size)\n\n    # Create tensor\n    input = torch.randn(world_size, num_elements, device=get_device(rank))\n    output = torch.empty(num_elements, device=get_device(rank))\n\n    # Warmup\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n\n    # All reduce\n    start_time = time.time()\n    dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()  # Wait for CUDA kerels to finish\n        dist.barrier()            # Wait for all the processes to get here\n    end_time = time.time()\n\n    duration = end_time - start_time\n    print(f\"[reduce_scatter] Rank {rank}: reduce_scatter(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")\n\n    # Estimate the bandwidth\n    data_bytes = output.element_size() * output.numel()  # How much data in the output\n    sent_bytes = data_bytes * (world_size - 1)  # How much needs to be sent\n    total_duration = world_size * duration  # Total time for transmission\n    bandwidth = sent_bytes / total_duration\n    print(f\"[reduce_scatter] Rank {rank}: reduce_scatter estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")\n\n    cleanup()\n\n\ndef data_parallelism():\n    text(\"## Distributed Data Parallel (DDP)\")\n    image(\"images/data-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets a slice of the data\")\n\n    # Generate data\n    batch_size = 128\n    num_dim = 1024\n    data = torch.randn(batch_size, num_dim)\n\n    spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)\n\n    text(\"Notes:\")\n    text(\"- Losses are different across nodes (computed on local data)\")\n    text(\"- Gradients are same, and therefore parameters are the same\")\n\n\ndef data_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_steps: int):\n    setup(rank, world_size)\n\n    # Get the slice of data for this rank\n    batch_size = data.size(0) // world_size  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n    start_index = rank * batch_size  # @inspect start_index\n    end_index = start_index + batch_size  # @inspect end_index\n    data = data[start_index:end_index].to(get_device(rank))\n\n    # Create MLP: # gelu(gelu(x @ params[0]) @ params[1]) ...\n    params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]\n    optimizer = torch.optim.AdamW(params, lr=1e-3)\n\n    for step in range(num_steps):\n        # Forward pass\n        x = data\n        for param in params:\n            x = x @ param\n            x = F.gelu(x)\n        loss = x.square().mean()  # Loss function is average squared magnitude\n\n        # Backward pass\n        loss.backward()\n\n        # Sync gradients across workers (NEW!)\n        if torch.cuda.is_available():\n            for param in params:\n                dist.all_reduce(tensor=param.grad, op=dist.ReduceOp.AVG, async_op=False)\n\n        # Update parameters\n        optimizer.step()\n\n        print(f\"[ddp] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\")\n\n    cleanup()\n\n\ndef tensor_parallelism():\n    image(\"images/tensor-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets part of each layer, transfer all data/activations\")\n\n    # Create data\n    batch_size = 128\n    num_dim = 1024\n    data = torch.randn(batch_size, num_dim)\n\n    spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)\n\n\ndef tensor_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int):\n    setup(rank, world_size)\n\n    # Note: no sharding of the data\n    data = data.to(get_device(rank))\n    batch_size = data.size(0)  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n    sharded_num_dim = num_dim // world_size  # Shard `num_dim`  @inspect sharded_num_dim\n\n    # Create model (each rank gets 1/world_size of the parameters)\n    params = [get_init_params(num_dim, sharded_num_dim, rank) for i in range(num_layers)]\n\n    # Forward pass\n    x = data\n    for i in range(num_layers):\n        # Compute activations (batch_size x sharded_num_dim)\n        x = x @ params[i]  # Note: this is only on a slice of the parameters\n        x = F.gelu(x)\n\n        # Allocate memory for activations (world_size x batch_size x sharded_num_dim)\n        activations = [torch.empty(batch_size, sharded_num_dim, device=get_device(rank)) for _ in range(world_size)]\n\n        # Send via all gather\n        dist.all_gather(tensor_list=activations, tensor=x, async_op=False)\n\n        # Just concatenate them to get (batch_size x num_dim)\n        x = torch.cat(activations, dim=1)\n\n    print(f\"Rank {rank}: forward pass produced activations {summarize_tensor(x)}\")\n\n    # Backward pass: homework exercise\n\n    cleanup()\n\n\ndef pipeline_parallelism():\n    image(\"images/pipeline-parallelism.png\", width=300)\n    text(\"Sharding strategy: each rank gets subset of layers, transfer all data/activations\")\n\n    # Create data\n    batch_size = 128\n    num_dim = 1024\n    data = torch.randn(batch_size, num_dim)\n\n    spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)\n\n\ndef pipeline_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_micro_batches: int):\n    setup(rank, world_size)\n\n    # All the data\n    data = data.to(get_device(rank))\n    batch_size = data.size(0)  # @inspect batch_size\n    num_dim = data.size(1)  # @inspect num_dim\n\n    # Split up layers\n    num_layers_per_rank = int_divide(num_layers, world_size)  # @inspect num_layers_per_rank\n    micro_batch_size = int_divide(batch_size, num_micro_batches)  # @inspect micro_batch_size\n\n    # Each rank gets a subset of layers\n    params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers_per_rank)]\n\n    # Forward pass\n\n    # Break up into micro batches to minimize the bubble\n    if rank == 0:\n        micro_batches = data.chunk(chunks=num_micro_batches, dim=0)  # The data\n    else:\n        micro_batches = [torch.empty(micro_batch_size, num_dim, device=get_device(rank)) for _ in range(num_micro_batches)]\n\n    for x in micro_batches:\n        # Get from previous rank\n        if rank - 1 >= 0:\n            dist.recv(tensor=x, src=rank - 1)\n\n        # Do the compute\n        for param in params:\n            x = x @ param\n            x = F.gelu(x)\n\n        print(f\"[pipeline] Rank {rank}: forward pass produced {summarize_tensor(x)}\")\n\n        # Send to the next rank\n        if rank + 1 < world_size:\n            dist.send(tensor=x, dst=rank + 1)\n\n    text(\"Not handled: overlapping communication/computation to eliminate pipeline bubbles\")\n\n    cleanup()\n\n############################################################\n\ndef setup(rank: int, world_size: int):\n    # This is where master lives (rank 0)\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"15623\"\n\n    if torch.cuda.is_available():\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    else:\n        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n\ndef cleanup():\n    torch.distributed.destroy_process_group()\n\n\ndef benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n    # Warmup: first times might be slower due to compilation, things not cached.\n    # Since we will run the kernel multiple times, the timing that matters is steady state.\n    for _ in range(num_warmups):\n        run()\n    torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n\n    # Time it for real now!\n    times: List[float] = []\n    for trial in range(num_trials):  # Do it multiple times to capture variance\n        start_time = time.time()\n\n        run()  # Actually perform computation\n        torch.cuda.synchronize()  # Wait for CUDA threads to finish (important!)\n\n        end_time = time.time()\n        times.append((end_time - start_time) * 1000)\n\n    mean_time = mean(times)\n    text(f\"{description}: {list(map(round1, sorted(times)))} (mean {round1(mean_time)} ms)\", pop_stack=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 13,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 14,
          "function_name": "main",
          "code": "text(\"Last week: parallelism within a single GPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last week: parallelism within a single GPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 15,
          "function_name": "main",
          "code": "text(\"This week: parallelism across multiple GPUs\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This week: parallelism across multiple GPUs",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 16,
          "function_name": "main",
          "code": "image(\"images/gpu-node-overview.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/gpu-node-overview.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 18,
          "function_name": "main",
          "code": "text(\"In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In both cases, **compute** (arithmetic logic units) is far from inputs/outputs (**data**).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 19,
          "function_name": "main",
          "code": "text(\"Unifying theme: orchestrate computation to avoid data transfer bottlenecks\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Unifying theme: orchestrate computation to avoid data transfer bottlenecks",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 21,
          "function_name": "main",
          "code": "text(\"Last week: reduce memory accesses via fusion/tiling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last week: reduce memory accesses via fusion/tiling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 22,
          "function_name": "main",
          "code": "text(\"This week: reduce communication across GPUs/nodes via replication/sharding\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This week: reduce communication across GPUs/nodes via replication/sharding",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 24,
          "function_name": "main",
          "code": "text(\"Generalized hierarchy (from small/fast to big/slow):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Generalized hierarchy (from small/fast to big/slow):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 25,
          "function_name": "main",
          "code": "text(\"- Single node, single GPU: L1 cache / shared memory\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, single GPU: L1 cache / shared memory",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 26,
          "function_name": "main",
          "code": "text(\"- Single node, single GPU: HBM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, single GPU: HBM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 27,
          "function_name": "main",
          "code": "text(\"- Single node, multi-GPU: NVLink\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Single node, multi-GPU: NVLink",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 28,
          "function_name": "main",
          "code": "text(\"- Multi-node, multi-GPU: NVSwitch\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Multi-node, multi-GPU: NVSwitch",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 30,
          "function_name": "main",
          "code": "text(\"## Part 1: building blocks of distributed communication/computation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Part 1: building blocks of distributed communication/computation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 53,
          "function_name": "hardware",
          "code": "def hardware():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 54,
          "function_name": "hardware",
          "code": "text(\"## Single GPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Single GPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 55,
          "function_name": "hardware",
          "code": "image(\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg\", width=1000)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-672bd77c57df485d07926615162a44d5-https_miro_medium_com_v2_resize_fit_2000_format_webp_1_6xoBKi5kL2dZpivFe1-zgw_jpeg",
          "style": {
            "width": 1000
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 56,
          "function_name": "hardware",
          "code": "text(\"Memory bandwidth for HBM for H100 NVL is 3.9 TB/s \"), article_link(\"https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Memory bandwidth for HBM for H100 NVL is 3.9 TB/s ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 58,
          "function_name": "hardware",
          "code": "text(\"## Multi-node, multi-GPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Multi-node, multi-GPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 60,
          "function_name": "hardware",
          "code": "text(\"Traditionally:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Traditionally:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 61,
          "function_name": "hardware",
          "code": "image(\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs42774-021-00098-3/MediaObjects/42774_2021_98_Fig1_HTML.png?as=webp\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-b0641f11a73711b3078acbd257b0c805-https_media_springernature_com_lw685_springer-static_image_art_3A10_1186_2Fs42774-021-00098-3_MediaObjects_42774_2021_98_Fig1_HTML_png_as_webp",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 62,
          "function_name": "hardware",
          "code": "text(\"- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) \"), article_link(\"https://en.wikipedia.org/wiki/PCI_Express\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GPUs on same node communicate via a PCI(e) bus (v7.0, 16 lanes => 242 GB/s) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/PCI_Express",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 63,
          "function_name": "hardware",
          "code": "text(\"- GPUs on different nodes communicate via Ethernet (~200 MB/sec)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- GPUs on different nodes communicate via Ethernet (~200 MB/sec)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 65,
          "function_name": "hardware",
          "code": "text(\"Both are too slow...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Both are too slow...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 66,
          "function_name": "hardware",
          "code": "text(\"Key hardware advance: have GPUs connect *directly*, bypassing CPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key hardware advance: have GPUs connect *directly*, bypassing CPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 68,
          "function_name": "hardware",
          "code": "text(\"## InfiniBand\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## InfiniBand",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 70,
          "function_name": "hardware",
          "code": "text(\"Standard developed in 1999; Mellanox created InfiniBand hardware, acquired by NVIDIA in 2019\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Standard developed in 1999; Mellanox created InfiniBand hardware, acquired by NVIDIA in 2019",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 71,
          "function_name": "hardware",
          "code": "text(\"Idea: Remote Direct Memory Access (RDMA) to connect nodes directly\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: Remote Direct Memory Access (RDMA) to connect nodes directly",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 72,
          "function_name": "hardware",
          "code": "image(\"https://lambdalabs.com/hubfs/Imported_Blog_Media/nvlink-diagram-update.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-1e382b3aee3e4d4f249a5b874e5f4173-https_lambdalabs_com_hubfs_Imported_Blog_Media_nvlink-diagram-update_png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 74,
          "function_name": "hardware",
          "code": "text(\"## NVLink/NVSwitch\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## NVLink/NVSwitch",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 76,
          "function_name": "hardware",
          "code": "text(\"NVIDIA developed proprietary protocol since 2014\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "NVIDIA developed proprietary protocol since 2014",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 77,
          "function_name": "hardware",
          "code": "text(\"4.5x more bandwidth than InfiniBand \"), article_link(\"https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "4.5x more bandwidth than InfiniBand ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 79,
          "function_name": "hardware",
          "code": "text(\"Within a node: NVLink connects GPUs directly, bypass CPU\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Within a node: NVLink connects GPUs directly, bypass CPU",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 80,
          "function_name": "hardware",
          "code": "image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/08/NVLink-generations-1.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-93caecbe56778b9bcde727f1cb88e848-https_developer-blogs_nvidia_com_wp-content_uploads_2022_08_NVLink-generations-1_png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 82,
          "function_name": "hardware",
          "code": "text(\"Across nodes: NVSwitch connects GPUs directly, bypass Ethernet\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Across nodes: NVSwitch connects GPUs directly, bypass Ethernet",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 83,
          "function_name": "hardware",
          "code": "image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/08/NVLink-all-to-all-connectivity-1.png\", width=800)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-61f8c89b5154e9da264362e9da6619f2-https_developer-blogs_nvidia_com_wp-content_uploads_2022_08_NVLink-all-to-all-connectivity-1_png",
          "style": {
            "width": 800
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 85,
          "function_name": "hardware",
          "code": "text(\"H100: 18 NVLink 4.0 links => 900GB/sec\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "H100: 18 NVLink 4.0 links => 900GB/sec",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 87,
          "function_name": "hardware",
          "code": "text(\"Let's check what our hardware setup is. \"), article_link(\"https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's check what our hardware setup is. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://guide.ncloud-docs.com/docs/en/server-baremetal-a100-check-vpc",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        },
        {
          "path": "lecture_08.py",
          "line_number": 89,
          "function_name": "hardware",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 31,
          "function_name": "main",
          "code": "hardware()                 # How nodes actually communicate"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 94,
          "function_name": "collective_operations",
          "code": "def collective_operations():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 95,
          "function_name": "collective_operations",
          "code": "text(\"Collective operations are the conceptual primitives used for distributed programming \"), article_link(\"https://en.wikipedia.org/wiki/Collective_operation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Collective operations are the conceptual primitives used for distributed programming ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": " [article]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://en.wikipedia.org/wiki/Collective_operation",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 97,
          "function_name": "collective_operations",
          "code": "text(\"- Collective means that specify communication pattern across many (e.g., 256) nodes\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Collective means that specify communication pattern across many (e.g., 256) nodes",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 98,
          "function_name": "collective_operations",
          "code": "text(\"- These are classic in the parallel programming literature from the 1980s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- These are classic in the parallel programming literature from the 1980s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 99,
          "function_name": "collective_operations",
          "code": "text(\"- For SIMD (Single Instruction, Multiple Data) parallelism\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- For SIMD (Single Instruction, Multiple Data) parallelism",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 100,
          "function_name": "collective_operations",
          "code": "text(\"- Better/faster abstraction than managing point-to-point communication yourself\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Better/faster abstraction than managing point-to-point communication yourself",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 102,
          "function_name": "collective_operations",
          "code": "text(\"Terminology:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Terminology:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 103,
          "function_name": "collective_operations",
          "code": "text(\"- **Rank**: a device (e.g., GPU)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Rank**: a device (e.g., GPU)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 104,
          "function_name": "collective_operations",
          "code": "text(\"- **World size**: number of devices\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **World size**: number of devices",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 106,
          "function_name": "collective_operations",
          "code": "text(\"## Broadcast\"), image(\"https://pytorch.org/tutorials/_images/broadcast.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Broadcast",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-525847c9d4b48933cb231204a2d13e0e-https_pytorch_org_tutorials__images_broadcast_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 108,
          "function_name": "collective_operations",
          "code": "text(\"## Scatter\"), image(\"https://pytorch.org/tutorials/_images/scatter.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Scatter",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-3aa3584628cb0526c8b0e9d02b15d876-https_pytorch_org_tutorials__images_scatter_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 110,
          "function_name": "collective_operations",
          "code": "text(\"## Gather\"), image(\"https://pytorch.org/tutorials/_images/gather.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-7e8670a3b7cdc7848394514ef1da090a-https_pytorch_org_tutorials__images_gather_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 112,
          "function_name": "collective_operations",
          "code": "text(\"## Reduce\"), image(\"https://pytorch.org/tutorials/_images/reduce.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Reduce",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-1c451df4406aea85e640d1ae7df6df31-https_pytorch_org_tutorials__images_reduce_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 114,
          "function_name": "collective_operations",
          "code": "text(\"## All-gather\"), image(\"https://pytorch.org/tutorials/_images/all_gather.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## All-gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-4a48977cd9545f897942a4a4ef1175ac-https_pytorch_org_tutorials__images_all_gather_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 116,
          "function_name": "collective_operations",
          "code": "text(\"## Reduce-scatter\"), image(\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Reduce-scatter",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-66ea136cfe7f3e7394fd0b056fd9d949-https_docs_nvidia_com_deeplearning_nccl_user-guide_docs__images_reducescatter_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 118,
          "function_name": "collective_operations",
          "code": "text(\"## All-reduce = reduce-scatter + all-gather\"), image(\"https://pytorch.org/tutorials/_images/all_reduce.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## All-reduce = reduce-scatter + all-gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "image",
          "data": "var/files/image-0ef9693f0008d5a75aa5ac2b542b83ac-https_pytorch_org_tutorials__images_all_reduce_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 120,
          "function_name": "collective_operations",
          "code": "text(\"Way to remember:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Way to remember:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 121,
          "function_name": "collective_operations",
          "code": "text(\"- Reduce: performs some associative/commutative operation (sum, min, max)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reduce: performs some associative/commutative operation (sum, min, max)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 122,
          "function_name": "collective_operations",
          "code": "text(\"- Broadcast/scatter is inverse of gather\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Broadcast/scatter is inverse of gather",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        },
        {
          "path": "lecture_08.py",
          "line_number": 123,
          "function_name": "collective_operations",
          "code": "text(\"- All: means destination is all devices\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- All: means destination is all devices",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 32,
          "function_name": "main",
          "code": "collective_operations()    # Conceptual programming interface"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 126,
          "function_name": "torch_distributed",
          "code": "def torch_distributed():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 127,
          "function_name": "torch_distributed",
          "code": "text(\"## PyTorch distributed library (`torch.distributed`) \")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## PyTorch distributed library (`torch.distributed`) ",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 128,
          "function_name": "torch_distributed",
          "code": "link(\"[Documentation]\", url=\"https://pytorch.org/docs/stable/distributed.html\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "[Documentation]",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 130,
          "function_name": "torch_distributed",
          "code": "text(\"- Provides clean interface for collective operations (e.g., `all_reduce`)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Provides clean interface for collective operations (e.g., `all_reduce`)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 131,
          "function_name": "torch_distributed",
          "code": "text(\"- Backends: gloo (CPU), nccl (GPU)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Backends: gloo (CPU), nccl (GPU)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 132,
          "function_name": "torch_distributed",
          "code": "text(\"- Also supports higher-level abstractions (e.g., `FullyShardedDataParallel`)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Also supports higher-level abstractions (e.g., `FullyShardedDataParallel`)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 134,
          "function_name": "torch_distributed",
          "code": "text(\"## NVIDIA Collective Communication Library (NCCL)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## NVIDIA Collective Communication Library (NCCL)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 136,
          "function_name": "torch_distributed",
          "code": "text(\"NCCL translates collective operations into low-level packets that are sent between GPUs. \"), link(title=\"[talk]\", url=\"https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "NCCL translates collective operations into low-level packets that are sent between GPUs. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[talk]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31880/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 137,
          "function_name": "torch_distributed",
          "code": "text(\"- Detect toplogy of hardware (e.g., number of nodes, switches, NVLink/PCIe)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Detect toplogy of hardware (e.g., number of nodes, switches, NVLink/PCIe)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 138,
          "function_name": "torch_distributed",
          "code": "text(\"- Optimize the path between ranks; ring (good bandwidth), tree (good latency)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Optimize the path between ranks; ring (good bandwidth), tree (good latency)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 139,
          "function_name": "torch_distributed",
          "code": "image(\"https://developer-blogs.nvidia.com/wp-content/uploads/2019/02/DBtree.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-55a1ae65c5ec0b09cd68cc4a81546f5f-https_developer-blogs_nvidia_com_wp-content_uploads_2019_02_DBtree_png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 140,
          "function_name": "torch_distributed",
          "code": "text(\"- Launches CUDA kernels to send/receive data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Launches CUDA kernels to send/receive data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 143,
          "function_name": "torch_distributed",
          "code": "link(title=\"[stdout]\", url=\"var/lecture_08_stdout.txt\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[stdout]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/lecture_08_stdout.txt",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 147,
          "function_name": "collective_operations_main",
          "code": "def collective_operations_main(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 151,
          "function_name": "collective_operations_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 154,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()  # Waits for all processes to get to this point"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 156,
          "function_name": "collective_operations_main",
          "code": "tensor = torch.tensor([0., 1, 2, 3], device=get_device(rank)) + rank  # Both input and output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 158,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before all-reduce]: {tensor}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 159,
          "function_name": "collective_operations_main",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)  # Modifies tensor in place"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 160,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after all-reduce]: {tensor}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 163,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 165,
          "function_name": "collective_operations_main",
          "code": "input = torch.arange(world_size, dtype=torch.float32, device=get_device(rank)) + rank  # Input"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 166,
          "function_name": "collective_operations_main",
          "code": "output = torch.empty(1, device=get_device(rank))  # Allocate output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 168,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before reduce-scatter]: input = {input}, output = {output}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 169,
          "function_name": "collective_operations_main",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 170,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after reduce-scatter]: input = {input}, output = {output}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 173,
          "function_name": "collective_operations_main",
          "code": "dist.barrier()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 175,
          "function_name": "collective_operations_main",
          "code": "input = output  # Input is the output of reduce-scatter"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 176,
          "function_name": "collective_operations_main",
          "code": "output = torch.empty(world_size, device=get_device(rank))  # Allocate output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 178,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [before all-gather]: input = {input}, output = {output}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 179,
          "function_name": "collective_operations_main",
          "code": "dist.all_gather_into_tensor(output_tensor=output, input_tensor=input, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 180,
          "function_name": "collective_operations_main",
          "code": "print(f\"Rank {rank} [after all-gather]: input = {input}, output = {output}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 182,
          "function_name": "collective_operations_main",
          "code": "text(\"Recall that all-reduce = reduce-scatter + all-gather!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that all-reduce = reduce-scatter + all-gather!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 184,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 184,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 184,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 184,
          "function_name": "collective_operations_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        },
        {
          "path": "lecture_08.py",
          "line_number": 144,
          "function_name": "torch_distributed",
          "code": "spawn(collective_operations_main, world_size=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 33,
          "function_name": "main",
          "code": "torch_distributed()        # How this is implemented in NCCL/PyTorch"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 187,
          "function_name": "benchmarking",
          "code": "def benchmarking():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 188,
          "function_name": "benchmarking",
          "code": "text(\"## Benchmarking\"), link(\"https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Benchmarking",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/stas00/ml-engineering/blob/master/network/benchmarks/all_reduce_bench.py",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 190,
          "function_name": "benchmarking",
          "code": "text(\"Let's see how fast commmunication happens (will restrict to just one node).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's see how fast commmunication happens (will restrict to just one node).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 192,
          "function_name": "benchmarking",
          "code": "text(\"### All-reduce\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### All-reduce",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 203,
          "function_name": "all_reduce",
          "code": "def all_reduce(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 207,
          "function_name": "all_reduce",
          "code": "tensor = torch.randn(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 210,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 211,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 216,
          "function_name": "all_reduce",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 217,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 218,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 221,
          "function_name": "all_reduce",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 223,
          "function_name": "all_reduce",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 224,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 227,
          "function_name": "all_reduce",
          "code": "size_bytes = tensor.element_size() * tensor.numel()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 228,
          "function_name": "all_reduce",
          "code": "sent_bytes = size_bytes * 2 * (world_size - 1)  # 2x because of send and receive"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 229,
          "function_name": "all_reduce",
          "code": "total_duration = world_size * duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 230,
          "function_name": "all_reduce",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 231,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 193,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=2, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 203,
          "function_name": "all_reduce",
          "code": "def all_reduce(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 204,
          "function_name": "all_reduce",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 207,
          "function_name": "all_reduce",
          "code": "tensor = torch.randn(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 210,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 211,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 216,
          "function_name": "all_reduce",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 217,
          "function_name": "all_reduce",
          "code": "dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 218,
          "function_name": "all_reduce",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 221,
          "function_name": "all_reduce",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 223,
          "function_name": "all_reduce",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 224,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 227,
          "function_name": "all_reduce",
          "code": "size_bytes = tensor.element_size() * tensor.numel()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 228,
          "function_name": "all_reduce",
          "code": "sent_bytes = size_bytes * 2 * (world_size - 1)  # 2x because of send and receive"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 229,
          "function_name": "all_reduce",
          "code": "total_duration = world_size * duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 230,
          "function_name": "all_reduce",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 231,
          "function_name": "all_reduce",
          "code": "print(f\"[all_reduce] Rank {rank}: all_reduce estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 233,
          "function_name": "all_reduce",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 194,
          "function_name": "benchmarking",
          "code": "spawn(all_reduce, world_size=4, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 196,
          "function_name": "benchmarking",
          "code": "text(\"### Reduce-scatter\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Reduce-scatter",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 236,
          "function_name": "reduce_scatter",
          "code": "def reduce_scatter(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 240,
          "function_name": "reduce_scatter",
          "code": "input = torch.randn(world_size, num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 241,
          "function_name": "reduce_scatter",
          "code": "output = torch.empty(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 245,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 250,
          "function_name": "reduce_scatter",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 255,
          "function_name": "reduce_scatter",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 257,
          "function_name": "reduce_scatter",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 258,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 261,
          "function_name": "reduce_scatter",
          "code": "data_bytes = output.element_size() * output.numel()  # How much data in the output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 262,
          "function_name": "reduce_scatter",
          "code": "sent_bytes = data_bytes * (world_size - 1)  # How much needs to be sent"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 263,
          "function_name": "reduce_scatter",
          "code": "total_duration = world_size * duration  # Total time for transmission"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 264,
          "function_name": "reduce_scatter",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 265,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 197,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=2, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 236,
          "function_name": "reduce_scatter",
          "code": "def reduce_scatter(rank: int, world_size: int, num_elements: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 237,
          "function_name": "reduce_scatter",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 240,
          "function_name": "reduce_scatter",
          "code": "input = torch.randn(world_size, num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 241,
          "function_name": "reduce_scatter",
          "code": "output = torch.empty(num_elements, device=get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 244,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 245,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 250,
          "function_name": "reduce_scatter",
          "code": "start_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 251,
          "function_name": "reduce_scatter",
          "code": "dist.reduce_scatter_tensor(output=output, input=input, op=dist.ReduceOp.SUM, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 252,
          "function_name": "reduce_scatter",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 255,
          "function_name": "reduce_scatter",
          "code": "end_time = time.time()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 257,
          "function_name": "reduce_scatter",
          "code": "duration = end_time - start_time"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 258,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter(world_size={world_size}, num_elements={num_elements}) took {round(duration * 1000)} ms\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 261,
          "function_name": "reduce_scatter",
          "code": "data_bytes = output.element_size() * output.numel()  # How much data in the output"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 262,
          "function_name": "reduce_scatter",
          "code": "sent_bytes = data_bytes * (world_size - 1)  # How much needs to be sent"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 263,
          "function_name": "reduce_scatter",
          "code": "total_duration = world_size * duration  # Total time for transmission"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 264,
          "function_name": "reduce_scatter",
          "code": "bandwidth = sent_bytes / total_duration"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 265,
          "function_name": "reduce_scatter",
          "code": "print(f\"[reduce_scatter] Rank {rank}: reduce_scatter estimated bandwidth = {round(bandwidth / 1024**3)} GB/sec\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 267,
          "function_name": "reduce_scatter",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 198,
          "function_name": "benchmarking",
          "code": "spawn(reduce_scatter, world_size=4, num_elements=1024**2)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        },
        {
          "path": "lecture_08.py",
          "line_number": 200,
          "function_name": "benchmarking",
          "code": "link(title=\"Reference on reasoning about operations\", url=\"https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Reference on reasoning about operations",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#allreduce",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 34,
          "function_name": "main",
          "code": "benchmarking()             # Estimate NCCL bandwidth"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 36,
          "function_name": "main",
          "code": "text(\"## Part 2: distributed training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Part 2: distributed training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 37,
          "function_name": "main",
          "code": "text(\"Walk through bare-bones implementations of each strategy on deep MLPs.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Walk through bare-bones implementations of each strategy on deep MLPs.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 38,
          "function_name": "main",
          "code": "text(\"Recall that MLPs are the compute bottleneck in Transformers.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that MLPs are the compute bottleneck in Transformers.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 270,
          "function_name": "data_parallelism",
          "code": "def data_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 271,
          "function_name": "data_parallelism",
          "code": "text(\"## Distributed Data Parallel (DDP)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Distributed Data Parallel (DDP)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 272,
          "function_name": "data_parallelism",
          "code": "image(\"images/data-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/data-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 273,
          "function_name": "data_parallelism",
          "code": "text(\"Sharding strategy: each rank gets a slice of the data\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets a slice of the data",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 276,
          "function_name": "data_parallelism",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 277,
          "function_name": "data_parallelism",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 278,
          "function_name": "data_parallelism",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 287,
          "function_name": "data_parallelism_main",
          "code": "def data_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_steps: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 288,
          "function_name": "data_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 291,
          "function_name": "data_parallelism_main",
          "code": "batch_size = data.size(0) // world_size  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 292,
          "function_name": "data_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 293,
          "function_name": "data_parallelism_main",
          "code": "start_index = rank * batch_size  # @inspect start_index"
        }
      ],
      "env": {
        "start_index": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 294,
          "function_name": "data_parallelism_main",
          "code": "end_index = start_index + batch_size  # @inspect end_index"
        }
      ],
      "env": {
        "end_index": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 295,
          "function_name": "data_parallelism_main",
          "code": "data = data[start_index:end_index].to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 298,
          "function_name": "data_parallelism_main",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 299,
          "function_name": "data_parallelism_main",
          "code": "optimizer = torch.optim.AdamW(params, lr=1e-3)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 301,
          "function_name": "data_parallelism_main",
          "code": "for step in range(num_steps):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 303,
          "function_name": "data_parallelism_main",
          "code": "x = data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 304,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 305,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 306,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 304,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 305,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 306,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 304,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 305,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 306,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 304,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 305,
          "function_name": "data_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 306,
          "function_name": "data_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 304,
          "function_name": "data_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 307,
          "function_name": "data_parallelism_main",
          "code": "loss = x.square().mean()  # Loss function is average squared magnitude"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 310,
          "function_name": "data_parallelism_main",
          "code": "loss.backward()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 313,
          "function_name": "data_parallelism_main",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 318,
          "function_name": "data_parallelism_main",
          "code": "optimizer.step()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 320,
          "function_name": "data_parallelism_main",
          "code": "print(f\"[ddp] Rank {rank}: step = {step}, loss = {loss.item()}, params = {[summarize_tensor(params[i]) for i in range(num_layers)]}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 301,
          "function_name": "data_parallelism_main",
          "code": "for step in range(num_steps):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 322,
          "function_name": "data_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 280,
          "function_name": "data_parallelism",
          "code": "spawn(data_parallelism_main, world_size=4, data=data, num_layers=4, num_steps=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 282,
          "function_name": "data_parallelism",
          "code": "text(\"Notes:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Notes:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 283,
          "function_name": "data_parallelism",
          "code": "text(\"- Losses are different across nodes (computed on local data)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Losses are different across nodes (computed on local data)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 284,
          "function_name": "data_parallelism",
          "code": "text(\"- Gradients are same, and therefore parameters are the same\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Gradients are same, and therefore parameters are the same",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 39,
          "function_name": "main",
          "code": "data_parallelism()         # Cut up along the batch dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 325,
          "function_name": "tensor_parallelism",
          "code": "def tensor_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 326,
          "function_name": "tensor_parallelism",
          "code": "image(\"images/tensor-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/tensor-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 327,
          "function_name": "tensor_parallelism",
          "code": "text(\"Sharding strategy: each rank gets part of each layer, transfer all data/activations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets part of each layer, transfer all data/activations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 330,
          "function_name": "tensor_parallelism",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 331,
          "function_name": "tensor_parallelism",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 332,
          "function_name": "tensor_parallelism",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 337,
          "function_name": "tensor_parallelism_main",
          "code": "def tensor_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 338,
          "function_name": "tensor_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 341,
          "function_name": "tensor_parallelism_main",
          "code": "data = data.to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 342,
          "function_name": "tensor_parallelism_main",
          "code": "batch_size = data.size(0)  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 128
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 343,
          "function_name": "tensor_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 344,
          "function_name": "tensor_parallelism_main",
          "code": "sharded_num_dim = num_dim // world_size  # Shard `num_dim`  @inspect sharded_num_dim"
        }
      ],
      "env": {
        "sharded_num_dim": 256
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 347,
          "function_name": "tensor_parallelism_main",
          "code": "params = [get_init_params(num_dim, sharded_num_dim, rank) for i in range(num_layers)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 350,
          "function_name": "tensor_parallelism_main",
          "code": "x = data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 353,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 354,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 357,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, sharded_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 360,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 363,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 353,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 354,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 357,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, sharded_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 360,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 363,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 353,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 354,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 357,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, sharded_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 360,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 363,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 353,
          "function_name": "tensor_parallelism_main",
          "code": "x = x @ params[i]  # Note: this is only on a slice of the parameters"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 354,
          "function_name": "tensor_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 357,
          "function_name": "tensor_parallelism_main",
          "code": "activations = [torch.empty(batch_size, sharded_num_dim, device=get_device(rank)) for _ in range(world_size)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 360,
          "function_name": "tensor_parallelism_main",
          "code": "dist.all_gather(tensor_list=activations, tensor=x, async_op=False)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 363,
          "function_name": "tensor_parallelism_main",
          "code": "x = torch.cat(activations, dim=1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 351,
          "function_name": "tensor_parallelism_main",
          "code": "for i in range(num_layers):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 365,
          "function_name": "tensor_parallelism_main",
          "code": "print(f\"Rank {rank}: forward pass produced activations {summarize_tensor(x)}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 369,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 369,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 369,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 369,
          "function_name": "tensor_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 334,
          "function_name": "tensor_parallelism",
          "code": "spawn(tensor_parallelism_main, world_size=4, data=data, num_layers=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 40,
          "function_name": "main",
          "code": "tensor_parallelism()       # Cut up along the width dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 372,
          "function_name": "pipeline_parallelism",
          "code": "def pipeline_parallelism():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 373,
          "function_name": "pipeline_parallelism",
          "code": "image(\"images/pipeline-parallelism.png\", width=300)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/pipeline-parallelism.png",
          "style": {
            "width": 300
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 374,
          "function_name": "pipeline_parallelism",
          "code": "text(\"Sharding strategy: each rank gets subset of layers, transfer all data/activations\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sharding strategy: each rank gets subset of layers, transfer all data/activations",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 377,
          "function_name": "pipeline_parallelism",
          "code": "batch_size = 128"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 378,
          "function_name": "pipeline_parallelism",
          "code": "num_dim = 1024"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 379,
          "function_name": "pipeline_parallelism",
          "code": "data = torch.randn(batch_size, num_dim)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 384,
          "function_name": "pipeline_parallelism_main",
          "code": "def pipeline_parallelism_main(rank: int, world_size: int, data: torch.Tensor, num_layers: int, num_micro_batches: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 429,
          "function_name": "setup",
          "code": "def setup(rank: int, world_size: int):"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 431,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_ADDR\"] = \"localhost\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 432,
          "function_name": "setup",
          "code": "os.environ[\"MASTER_PORT\"] = \"15623\""
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 434,
          "function_name": "setup",
          "code": "if torch.cuda.is_available():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 437,
          "function_name": "setup",
          "code": "dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 385,
          "function_name": "pipeline_parallelism_main",
          "code": "setup(rank, world_size)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 388,
          "function_name": "pipeline_parallelism_main",
          "code": "data = data.to(get_device(rank))"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 389,
          "function_name": "pipeline_parallelism_main",
          "code": "batch_size = data.size(0)  # @inspect batch_size"
        }
      ],
      "env": {
        "batch_size": 128
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 390,
          "function_name": "pipeline_parallelism_main",
          "code": "num_dim = data.size(1)  # @inspect num_dim"
        }
      ],
      "env": {
        "num_dim": 1024
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 393,
          "function_name": "pipeline_parallelism_main",
          "code": "num_layers_per_rank = int_divide(num_layers, world_size)  # @inspect num_layers_per_rank"
        }
      ],
      "env": {
        "num_layers_per_rank": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 394,
          "function_name": "pipeline_parallelism_main",
          "code": "micro_batch_size = int_divide(batch_size, num_micro_batches)  # @inspect micro_batch_size"
        }
      ],
      "env": {
        "micro_batch_size": 32
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 397,
          "function_name": "pipeline_parallelism_main",
          "code": "params = [get_init_params(num_dim, num_dim, rank) for i in range(num_layers_per_rank)]"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 402,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank == 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 403,
          "function_name": "pipeline_parallelism_main",
          "code": "micro_batches = data.chunk(chunks=num_micro_batches, dim=0)  # The data"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 407,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 417,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline] Rank {rank}: forward pass produced {summarize_tensor(x)}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 420,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 421,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 407,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 417,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline] Rank {rank}: forward pass produced {summarize_tensor(x)}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 420,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 421,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 407,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 417,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline] Rank {rank}: forward pass produced {summarize_tensor(x)}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 420,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 421,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 407,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 409,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank - 1 >= 0:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 414,
          "function_name": "pipeline_parallelism_main",
          "code": "x = x @ param"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 415,
          "function_name": "pipeline_parallelism_main",
          "code": "x = F.gelu(x)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 413,
          "function_name": "pipeline_parallelism_main",
          "code": "for param in params:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 417,
          "function_name": "pipeline_parallelism_main",
          "code": "print(f\"[pipeline] Rank {rank}: forward pass produced {summarize_tensor(x)}\")"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 420,
          "function_name": "pipeline_parallelism_main",
          "code": "if rank + 1 < world_size:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 421,
          "function_name": "pipeline_parallelism_main",
          "code": "dist.send(tensor=x, dst=rank + 1)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 407,
          "function_name": "pipeline_parallelism_main",
          "code": "for x in micro_batches:"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 423,
          "function_name": "pipeline_parallelism_main",
          "code": "text(\"Not handled: overlapping communication/computation to eliminate pipeline bubbles\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Not handled: overlapping communication/computation to eliminate pipeline bubbles",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 425,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 425,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 440,
          "function_name": "cleanup",
          "code": "def cleanup():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 425,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        },
        {
          "path": "lecture_08.py",
          "line_number": 441,
          "function_name": "cleanup",
          "code": "torch.distributed.destroy_process_group()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        },
        {
          "path": "lecture_08_utils.py",
          "line_number": 32,
          "function_name": "spawn",
          "code": "func(*args)"
        },
        {
          "path": "lecture_08.py",
          "line_number": 425,
          "function_name": "pipeline_parallelism_main",
          "code": "cleanup()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        },
        {
          "path": "lecture_08.py",
          "line_number": 381,
          "function_name": "pipeline_parallelism",
          "code": "spawn(pipeline_parallelism_main, world_size=2, data=data, num_layers=4, num_micro_batches=4)"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 41,
          "function_name": "main",
          "code": "pipeline_parallelism()     # Cut up along the depth dimension"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 43,
          "function_name": "main",
          "code": "text(\"## Summary\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Summary",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 45,
          "function_name": "main",
          "code": "text(\"The game: trading off\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The game: trading off",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 46,
          "function_name": "main",
          "code": "text(\"- memory usage (store locally) and\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- memory usage (store locally) and",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 47,
          "function_name": "main",
          "code": "text(\"- communication (send across GPUs)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- communication (send across GPUs)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 49,
          "function_name": "main",
          "code": "text(\"- Hardware is getting faster, but will always have this hierarchical structure\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Hardware is getting faster, but will always have this hierarchical structure",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "lecture_08.py",
          "line_number": 50,
          "function_name": "main",
          "code": "text(\"- Many ways to parallelize: data, tensor/expert, pipeline, sequence\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Many ways to parallelize: data, tensor/expert, pipeline, sequence",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}